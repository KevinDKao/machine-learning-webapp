{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09 K-nearest neighbors and Multinomial regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Lab!\n",
    "\n",
    "In general, there are three types of machine learning: supervised learning, unsupervised learning, and semi-supervised learning. In this lab, we will focus on supervised learning, which is the most commonly used type of machine learning. It involves training a model on a labeled dataset, where the desired output is already known for each input. The model learns to map inputs to outputs by minimizing the difference between its predicted output and the actual output. The ultimate goal is to use the trained model to make prediction on new data where $y$ is unknown. \n",
    "\n",
    "There are two main types of supervised learning: regression and classification. Regression involves predicting a continuous numerical value for a given input. For example, predicting the price of a house based on its features such as location, square footage, and number of bedrooms. Classification, on the other hand, involves predicting a categorical label or class for a given input. For example, to predict whether an email is spam or not spam based on its content.\n",
    "\n",
    "In this lab, you will:\n",
    "- Use the [`K nearest neighbors regression`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor)\n",
    "- Use the [`K nearest neighbors classifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)\n",
    "- Use [`multinomial regression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) (Note: in sklearn they call it logistic regression whether there are 2 classes or more)\n",
    "\n",
    "\n",
    "Please import specific component of `scikit-learn` when needed. For example: `from sklearn import neighbors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from interactive2 import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regression task\n",
    "In this part of the lab, we will be using $K$-nearest neighbors to perform regression. Here are two interactive demos that you can use to understand $K$-nearest neighbors regression better. We have a 1-D example (as done in class) and also a 2-D example which is an extension of the 1-D case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive $k$-nearest neighbors regression (with 1 covariatess)\n",
    "Remember from lectures that $k$-nearest neighbors regression (using 1 covariate) attempts to guess a continuous $y$ from (any) $x$ by **averaging** the $k$ points nearest to it. Let's try to pick $k$ to get an idea of how **$k$-nearest neighbor regression** works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b33915abf349719e54404ae3a066e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='k', max=50, min=1, step=0), IntSlider(value=50, descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knn_1d_reg().interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive $k$-nearest neighbors regression (with 2 covariatess)\n",
    "Remember from lectures that $k$-nearest neighbors regression (using 2 covariates) attempts to guess a continuous $y$ from (any) $x_1$ and $x_2$ by **averaging** the $k$ points nearest to it. Let's try to pick $k$ to get an idea of how **$k$-nearest neighbor regression** works in 2D. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b219c0b9ad476c9f3a765421e322dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='k', max=20, min=1, step=0), IntSlider(value=50, descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knn_2d_reg().interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get some data\n",
    "\n",
    "This lab will be a little different in that you will be getting your own data! Go to The University of California at Urvine has a very nice [repository of datasets](https://archive.ics.uci.edu/datasets) that you can look through. Find a dataset that you are interested in which you can perform regression on (Hint: use the filters to narrow down your options). Additionally, a dataset in a .csv format is preferred as you know how to load those in, but I won't stop you if you want to try to load other datasets. In fact some datasets have a .data extension (or something like that) but when you open them up in notepad (Windows) or TextEdit (Mac) you see that they are in a CSV format. The only thing to be careful of is that some datasets are not separated by commas but rather by other characters, such as semicolons (`';'`), spaces (`' '`), or tabs ('\\t'). To use these datasets, use the `delimiter` parameter in your pandas function call. **Do not pick the same dataset as your neighbor!**\n",
    "\n",
    "\n",
    "<div style=\"background-color:rgba(0, 255, 0, 0.15);\">\n",
    "\n",
    "**Question 1.** \n",
    "1. Load in the dataset and get some summary statistics for each variable.\n",
    "2. Additionally, use `df.corr` to see if any features are correlated with the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer to Q1.1\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer to Q1.2\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 255, 0, 0.15);\">\n",
    "\n",
    "**Question 2.** Plot some of the variables against the response variable $y$ and choose a 3 that you think can help you predict $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer to Q2\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0LNbB_USagWG"
   },
   "source": [
    "<div style=\"background-color:rgba(0, 255, 0, 0.15);\">\n",
    "\n",
    "**Question 3.** Use [`K nearest neighbors regression`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor) on one of your 3 variables to predict $y$. Remember that $x$ should be a DataFrame and not a data series for sklearn models to work!\n",
    "1. Plot a scatter plot of $x$ vs $y$ like you did in the previous question, but this time add the $K$-nearest neighbor prediction line. How do you do this?\n",
    "   - Well define a new variable `x_line` which goes across your entire plot. For example, if your x-axis goes form -10 to 10, you should have `x_line` be an array-like object that looks like `[-10, -9.8, -9.6, ..., 9.6, 9.8, 10]`.\n",
    "   - Then, define `y_line` to be the model predictions of each value in `x_line` using your $K$-NN model\n",
    "   - Plot `x_line` vs `y_line` on the same plot as your data\n",
    "   - Adjust $K$ to get the best looking line (you can eyeball it)\n",
    "3. Repeat for the other one of the 2 variables\n",
    "4. Repeat for the last variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer to Q3.1\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer to Q3.2\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer to Q3.3\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification task\n",
    "\n",
    "In this part of the lab, we will be using $K$-nearest neighbors and multinomial regression to perform classification. Here are four interactive demos that you can use to understand $K$-nearest neighbors classifcation and multinomial regression better. Just as with the regression, we also a 2-D examples here which are extensions of the 1-D case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive $k$-nearest neighbors classification (with 1 covariatess)\n",
    "Remember from lectures that $k$-nearest neighbors classification (using 1 covariate) attempts to guess a categorical $y$ from (any) $x$ by taking the most **votes** of the $k$ points nearest to it. Let's try to pick $k$ to get an idea of how **$k$-nearest neighbor regression** works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4790b24f6e13444cbaeea9ff0918a81c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='k', max=20, min=1, step=0), IntSlider(value=50, descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knn_1d_class().interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive $k$-nearest neighbors classification (with 2 covariatess)\n",
    "Remember from lectures that $k$-nearest neighbors classification (using 2 covariates) attempts to guess a categorical $y$ from (any) $x_1$ and $x_2$ by taking the most **votes** of the $k$ points nearest to it. Let's try to pick $k$ to get an idea of how **$k$-nearest neighbor classification** works in 2D. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dad26a176a7419590404fa7ad850472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='k', max=20, min=1, step=0), IntSlider(value=50, descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knn_2d_class().interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive multinomial regression (with 1 covariates)\n",
    "Remember from lectures that the formula for logistic regression with 2 covariates is:\n",
    "$$\n",
    "f(x, 0) = \\frac{1}{1 + e^{c_0x + b_0} + e^{c_1x + b_1}} \\approx \\text{Probability that } y=0;\n",
    "$$\n",
    "$$\n",
    "f(x, 1) = \\frac{e^{c_0x + b_0}}{1 + e^{c_0x + b_0} + e^{c_1x + b_1}}  \\approx \\text{Probability that } y=1;\n",
    "$$\n",
    "$$\n",
    "f(x, 2) = \\frac{e^{c_1x + b_1}}{1 + e^{c_0x + b_0} + e^{c_1x + b_1}}  \\approx \\text{Probability that } y=2;\n",
    "$$\n",
    "where the second argument (the 0, 1, and 2) specifies which class we want to get the probabiltiy from. Let's try to pick $c_0, c_1, b_0$ and $b_1$ manually to get an idea of how **multinomial regression** works. When you actually use `sklearn` this is done automatically using built-in mathematics (which is based on multivariate calculus and linear algebra).## Interactive multinomial regression (with 1 covariates)\n",
    "Remember from lectures that the formula for logistic regression with 2 covariates is:\n",
    "$$\n",
    "f(x, 0) = \\frac{1}{1 + e^{c_0x + b_0} + e^{c_1x + b_1}} \\approx \\text{Probability that } y=0;\n",
    "$$\n",
    "$$\n",
    "f(x, 1) = \\frac{e^{c_0x + b_0}}{1 + e^{c_0x + b_0} + e^{c_1x + b_1}}  \\approx \\text{Probability that } y=1;\n",
    "$$\n",
    "$$\n",
    "f(x, 2) = \\frac{e^{c_1x + b_1}}{1 + e^{c_0x + b_0} + e^{c_1x + b_1}}  \\approx \\text{Probability that } y=2;\n",
    "$$\n",
    "where the second argument (the 0, 1, and 2) specifies which class we want to get the probabiltiy from. Let's try to pick $c_0, c_1, b_0$ and $b_1$ manually to get an idea of how **multinomial regression** works. When you actually use `sklearn` this is done automatically using built-in mathematics (which is based on multivariate calculus and linear algebra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e39d1bc97824240b857d56c5b214d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='c0', max=50.0, min=-50.0), FloatSlider(value=0.0, de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multinomial1().interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive multinomial regression (with 2 covariatess)\n",
    "Remember from lectures that the formula for logistic regression with 2 covariates is:\n",
    "$$\n",
    "f(x_1, x_2, 0) = \\frac{1}{1 + e^{d_0x_1 + c_0x_2 + b_0} + e^{d_1x_1 + c_1x_2 + b_1}} \\approx \\text{Probability that } y=0;\n",
    "$$\n",
    "$$\n",
    "f(x_1, x_2, 1) = \\frac{e^{d_0x_1 + c_0x_2 + b_0}}{1 + e^{d_0x_1 + c_0x_2 + b_0} + e^{d_1x_1 + c_1x_2 + b_1}}  \\approx \\text{Probability that } y=1;\n",
    "$$\n",
    "$$\n",
    "f(x_1, x_2, 2) = \\frac{e^{d_1x_1 + c_1x_2 + b_1}}{1 + e^{d_0x_1 + c_0x_2 + b_0} + e^{d_1x_1 + c_1x_2 + b_1}}  \\approx \\text{Probability that } y=2;\n",
    "$$\n",
    "where the third argument (the 0, 1, and 2) specifies which class we want to get the probabiltiy from. Let's try to pick $d_0, d_1, c_0, c_1, b_0$ and $b_1$ manually to get an idea of how **multinomial regression** works in 2D. When you actually use `sklearn` this is done automatically using built-in mathematics (which is based on multivariate calculus and linear algebra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fa5f34e79a48768620e37dfbb59220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='d0', max=10.0, min=-10.0), FloatSlider(value=0.0, de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multinomial2().interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get some data\n",
    "\n",
    "Find another dataset that you are interested in which you can perform classification on.\n",
    "\n",
    "<div style=\"background-color:rgba(0, 255, 0, 0.15);\">\n",
    "\n",
    "**Question 4.** Load in the dataset and get some summary statistics for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer to Q4\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 255, 0, 0.15);\">\n",
    "\n",
    "**Question 5.** You will now need to plot 2 variables on a scatter plot, which are colored differently for each class in $y$. Do this for variables that you think separate the classes well. (Hint: If you don't remember how to do this, we did this in Demo 08 in class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer to Q5\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 255, 0, 0.15);\">\n",
    "\n",
    "**Question 6.** \n",
    "1. Perform [`K nearest neighbors classification`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) on these two variables to try to predict $y$\n",
    "2. Calculate your training accuracy by writing a function with a loop in it to calculate how many points are correctly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer to Q6.1\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer to Q6.2\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 255, 0, 0.15);\">\n",
    "\n",
    "**Question 7.** Perform [`multinomial regression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) instead of $K$-nearest neighbors and repeat Q6.1 and Q6.2. (Hint: if you wrote your function well for 6.2, you can just call it here instead of defining a new one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer to Q7.1\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer to Q7.2\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you're done with the lab!  Be sure to...\n",
    "\n",
    "* **Save** from the File menu,\n",
    "* **Review** the lab so that you understand each line!\n",
    "* **Shut down your kernel** from the Kernel menu,\n",
    "* **Rename your ipynb file**, replacing LASTNAMES with your last names\n",
    "* **Get your file ready** for grading during next lab session. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
